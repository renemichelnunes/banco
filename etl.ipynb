{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Configuração<h1>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import upper\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import date_format\n",
    "from pyspark.sql.functions import year, month, dayofmonth\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.sql.functions import sum\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.functions import col, format_number\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Configurar a sessão do Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ETL\") \\\n",
    "    .config(\"spark.jars\", \"postgresql-8.2-506.jdbc3.jar\") \\\n",
    "    .getOrCreate()\n",
    "sqlContext = SQLContext(spark)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Extração<h1>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obter os data frames das tabelas\n",
    "query_products = \"select * from products\"\n",
    "query_categories = \"select * from categories\"\n",
    "query_suppliers = \"select * from suppliers\"\n",
    "query_sales_items = \"select * from sales_items\"\n",
    "query_sales = \"select * from sales\"\n",
    "query_sellers = \"select * from sellers\"\n",
    "query_customers = \"select * from customers\"\n",
    "\n",
    "\n",
    "df_products = sqlContext.read.format('jdbc').options(\n",
    "    url='jdbc:postgresql://localhost/fatorv',\n",
    "    dbtable='({}) as products'.format(query_products),\n",
    "    user='fatorv',\n",
    "    password='123456',\n",
    "    driver='org.postgresql.Driver').load()\n",
    "\n",
    "df_suppliers = sqlContext.read.format('jdbc').options(\n",
    "    url='jdbc:postgresql://localhost/fatorv',\n",
    "    dbtable='({}) as suppliers'.format(query_suppliers),\n",
    "    user='fatorv',\n",
    "    password='123456',\n",
    "    driver='org.postgresql.Driver').load()\n",
    "\n",
    "df_categories = sqlContext.read.format('jdbc').options(\n",
    "    url='jdbc:postgresql://localhost/fatorv',\n",
    "    dbtable='({}) as categories'.format(query_categories),\n",
    "    user='fatorv',\n",
    "    password='123456',\n",
    "    driver='org.postgresql.Driver').load()\n",
    "\n",
    "df_sales_items = sqlContext.read.format('jdbc').options(\n",
    "    url='jdbc:postgresql://localhost/fatorv',\n",
    "    dbtable='({}) as sales_items'.format(query_sales_items),\n",
    "    user='fatorv',\n",
    "    password='123456',\n",
    "    driver='org.postgresql.Driver').load()\n",
    "\n",
    "df_sales = sqlContext.read.format('jdbc').options(\n",
    "    url='jdbc:postgresql://localhost/fatorv',\n",
    "    dbtable='({}) as sales'.format(query_sales),\n",
    "    user='fatorv',\n",
    "    password='123456',\n",
    "    driver='org.postgresql.Driver').load()\n",
    "\n",
    "df_sellers = sqlContext.read.format('jdbc').options(\n",
    "    url='jdbc:postgresql://localhost/fatorv',\n",
    "    dbtable='({}) as sellers'.format(query_sellers),\n",
    "    user='fatorv',\n",
    "    password='123456',\n",
    "    driver='org.postgresql.Driver').load()\n",
    "\n",
    "df_customers = sqlContext.read.format('jdbc').options(\n",
    "    url='jdbc:postgresql://localhost/fatorv',\n",
    "    dbtable='({}) as customers'.format(query_customers),\n",
    "    user='fatorv',\n",
    "    password='123456',\n",
    "    driver='org.postgresql.Driver').load()\n",
    "\n",
    "states_schema = StructType([\n",
    "    StructField(\"id_uf\", IntegerType(), False),\n",
    "    StructField(\"sigla_uf\", StringType(), False),\n",
    "    StructField(\"state_code\", StringType(), False),\n",
    "    StructField(\"nome_uf\", StringType(), False),\n",
    "    StructField(\"id_regiao\", IntegerType(), False)\n",
    "])\n",
    "\n",
    "regions_schema = StructType([\n",
    "    StructField(\"id_regiao\", IntegerType(), False),\n",
    "    StructField(\"sigla_regiao\", StringType(), False),\n",
    "    StructField(\"nome_regiao\", StringType(), False)\n",
    "])\n",
    "\n",
    "# Tabela regions\n",
    "df_regions = spark.read.option(\"multiline\", \"true\").schema(regions_schema).json(\"regioes.json\")\n",
    "df_regions = df_regions.withColumnRenamed(\"id_regiao\", \"region_id\") \\\n",
    "                     .withColumnRenamed(\"sigla_regiao\", \"region_acronym\") \\\n",
    "                     .withColumnRenamed(\"nome_regiao\", \"region_name\")\n",
    "\n",
    "# Tabela states\n",
    "df_states = spark.read.option(\"multiline\", \"true\").schema(states_schema).json(\"estados.json\")\n",
    "df_states = df_states.withColumnRenamed(\"id_uf\", \"state_id\") \\\n",
    "                     .withColumnRenamed(\"sigla_uf\", \"state_acronym\") \\\n",
    "                     .withColumnRenamed(\"nome_uf\", \"state_name\") \\\n",
    "                     .withColumnRenamed(\"id_regiao\", \"region_id\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Transformação<h1>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sales_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- date_id: integer (nullable = false)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- seller_id: integer (nullable = true)\n",
      " |-- total_price: decimal(10,2) (nullable = true)\n",
      " |-- supplier_id: integer (nullable = true)\n",
      " |-- state_id: integer (nullable = true)\n",
      " |-- category_id: integer (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- sell_price: decimal(10,2) (nullable = true)\n",
      " |-- sub_total: decimal(10,2) (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Alterando os dados das tabelas, construindo a tabela fatos\n",
    "\n",
    "# Join states com regions\n",
    "df_joined_st_re = df_states.join(df_regions, \"region_id\")\n",
    "#df_joined_st_re.show()\n",
    "\n",
    "# Fazendo upper em supplier_name e email em suppliers\n",
    "df_suppliers = df_suppliers.withColumn(\"email\", upper(df_suppliers[\"email\"])) \\\n",
    "                           .withColumn(\"supplier_name\", upper(df_suppliers[\"supplier_name\"]))\n",
    "\n",
    "# Divide a date em year, month, day\n",
    "df_sales = df_sales.withColumn(\"year\", year(\"date\")) \\\n",
    "                   .withColumn(\"month\", month(\"date\")) \\\n",
    "                   .withColumn(\"day\", dayofmonth(\"date\"))\n",
    "\n",
    "# Muda o formato da data\n",
    "df_sales = df_sales.withColumn(\"date\", date_format(\"date\", \"yyyyMMdd\"))\n",
    "\n",
    "# Criar df_date\n",
    "df_date = df_sales.select(\"date\", \"year\", \"month\", \"day\").distinct()\n",
    "df_date = df_sales.select(\"date\", \"year\", \"month\", \"day\").distinct() \\\n",
    "                  .withColumn(\"date_id\", monotonically_increasing_id() + 1)\n",
    "df_date = df_date.withColumn(\"date\", col(\"date\").cast(\"integer\")) \\\n",
    "                 .withColumn(\"date_id\", col(\"date_id\").cast(\"integer\"))\n",
    "\n",
    "\n",
    "# Criar coluna quarter(trimestre)\n",
    "quarter = when(df_date[\"month\"].between(1,3), 1) \\\n",
    "          .when(df_date[\"month\"].between(4,6), 2) \\\n",
    "          .when(df_date[\"month\"].between(7,9), 3) \\\n",
    "          .otherwise(4)\n",
    "df_date = df_date.withColumn(\"quarter\", quarter)\n",
    "#df_date.show()\n",
    "\n",
    "# Join df_sales com df_date\n",
    "df_sales = df_sales.join(df_date, \"date\")\n",
    "df_sales = df_sales.select(\"sales_id\", \"customer_id\", \"seller_id\", \"date_id\", \"total_price\")\n",
    "#df_sales.show()\n",
    "\n",
    "# Join suppliers com states, troca a coluna states por states_id\n",
    "df_joined_sup_st = df_suppliers.join(df_states, df_suppliers[\"state\"] == df_states[\"state_acronym\"], \"inner\")\n",
    "df_suppliers = df_joined_sup_st.select(\"supplier_id\", \"supplier_name\", \"email\",\"state_id\")\n",
    "#df_suppliers.show()\n",
    "\n",
    "# Fazendo upper em supplier_name e email em sellers\n",
    "df_sellers = df_sellers.withColumn(\"email\", upper(df_sellers[\"email\"])) \\\n",
    "                       .withColumn(\"seller_name\", upper(df_sellers[\"seller_name\"]))\n",
    "\n",
    "# Join sellers com states, troca a coluna states por states_id\n",
    "df_joined_se_st = df_sellers.join(df_states, df_sellers[\"state\"] == df_states[\"state_acronym\"], \"inner\")\n",
    "df_sellers = df_joined_se_st.select(\"seller_id\", \"seller_name\", \"email\", \"tx_commission\",\"state_id\")\n",
    "#df_sellers.show()\n",
    "\n",
    "# Fazendo upper em supplier_name e email em customers\n",
    "df_customers = df_customers.withColumn(\"email\", upper(df_customers[\"email\"])) \\\n",
    "                           .withColumn(\"customer_name\", upper(df_customers[\"customer_name\"]))\n",
    "\n",
    "# Join customers com states, troca a coluna states por states_id\n",
    "df_joined_cu_st = df_customers.join(df_states, df_customers[\"state\"] == df_states[\"state_acronym\"], \"inner\")\n",
    "df_customers = df_joined_cu_st.select(\"customer_id\", \"customer_name\", \"email\",\"state_id\")\n",
    "#df_customers.show()\n",
    "\n",
    "# Fazendo upper em product_name em products\n",
    "df_products = df_products.withColumn(\"product_name\", upper(df_products[\"product_name\"]))\n",
    "\n",
    "# Join entre products e suppliers\n",
    "df_products = df_products.drop(\"price\")\n",
    "df_joined_products_suppliers = df_products.join(df_suppliers, \"supplier_id\")\n",
    "#df_joined_products_suppliers.show()\n",
    "\n",
    "# Fazendo upper em category_name em categories\n",
    "df_categories = df_categories.withColumn(\"category_name\", upper(df_categories[\"category_name\"]))\n",
    "\n",
    "# Join com categories\n",
    "df_joined_products_suppliers_categories = df_joined_products_suppliers.join(df_categories, \"category_id\")\n",
    "#df_joined_products_suppliers_categories.show()\n",
    "\n",
    "#Join com sales_items\n",
    "df_sales_items = df_sales_items.withColumn(\"sell_price\", df_sales_items[\"price\"])\n",
    "df_joined_prod_sup_cat_si = df_joined_products_suppliers_categories.join(df_sales_items, \"product_id\")\n",
    "#df_joined_prod_sup_cat_si.show()\n",
    "\n",
    "#join com sales\n",
    "df_joined_prod_sup_cat_si_sa = df_joined_prod_sup_cat_si.join(df_sales, \"sales_id\")\n",
    "#df_joined_prod_sup_cat_si_sa.show()\n",
    "\n",
    "# Calculando sub_total = sell_price * quantity\n",
    "df_joined_prod_sup_cat_si_sa = df_joined_prod_sup_cat_si_sa.withColumn(\"sub_total\", col(\"sell_price\") * col(\"quantity\"))\n",
    "\n",
    "# Calculando total_price = sum(sub_total)\n",
    "df_joined_prod_sup_cat_si_sa.createOrReplaceTempView(\"sales_data\")\n",
    "result = spark.sql(\"\"\"\n",
    "    select \n",
    "        sales_id,\n",
    "        sum(sub_total) as total_price_2\n",
    "    from\n",
    "        sales_data\n",
    "    group by\n",
    "        sales_id\n",
    "\"\"\")\n",
    "df_joined_prod_sup_cat_si_sa = df_joined_prod_sup_cat_si_sa.join(result, \"sales_id\", \"left_outer\") \\\n",
    ".withColumn(\"total_price\", result[\"total_price_2\"])\n",
    "\n",
    "df_joined_prod_sup_cat_si_sa = df_joined_prod_sup_cat_si_sa.select(\"sales_id\", \"product_id\", \"date_id\", \"customer_id\", \"seller_id\", \n",
    "                                                                   \"total_price\", \"supplier_id\", \"state_id\", \"category_id\", \"quantity\", \n",
    "                                                                   \"sell_price\", \"sub_total\")\n",
    "df_joined_prod_sup_cat_si_sa = df_joined_prod_sup_cat_si_sa.withColumn(\"total_price\", col(\"total_price\").cast(\"decimal(10,2)\"))\n",
    "df_joined_prod_sup_cat_si_sa = df_joined_prod_sup_cat_si_sa.withColumn(\"sell_price\", col(\"sell_price\").cast(\"decimal(10,2)\"))\n",
    "df_joined_prod_sup_cat_si_sa = df_joined_prod_sup_cat_si_sa.withColumn(\"sub_total\", col(\"sub_total\").cast(\"decimal(10,2)\"))\n",
    "df_joined_prod_sup_cat_si_sa = df_joined_prod_sup_cat_si_sa.withColumn(\"date_id\", col(\"date_id\").cast(\"integer\"))\n",
    "#df_joined_prod_sup_cat_si_sa.show()\n",
    "#df_joined_prod_sup_cat_si_sa.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Carga<h1>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o932.jdbc.\n: org.postgresql.util.PSQLException: ERROR: cannot drop table dim_categories because other objects depend on it\n  Detalhe: constraint dim_categories_fato_sales_items_fk on table fato_sales_items depends on table dim_categories\n  Dica: Use DROP ... CASCADE to drop the dependent objects too.\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2713)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2401)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:368)\n\tat org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:498)\n\tat org.postgresql.jdbc.PgStatement.execute(PgStatement.java:415)\n\tat org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:335)\n\tat org.postgresql.jdbc.PgStatement.executeCachedSql(PgStatement.java:321)\n\tat org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:297)\n\tat org.postgresql.jdbc.PgStatement.executeUpdate(PgStatement.java:270)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.executeStatement(JdbcUtils.scala:1094)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.dropTable(JdbcUtils.scala:81)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:63)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:756)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m postgres_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjdbc:postgresql://localhost:5432/fatorvgestao2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m properties \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfatorv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassword\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m123456\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdriver\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.postgresql.Driver\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m : postgres_url\n\u001b[1;32m      7\u001b[0m }\n\u001b[0;32m----> 9\u001b[0m \u001b[43mdf_categories\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpostgres_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdim_categories\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperties\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m df_customers\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mjdbc(postgres_url, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdim_customers\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m, properties)\n\u001b[1;32m     11\u001b[0m df_sellers\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mjdbc(postgres_url, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdim_sellers\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m, properties)\n",
      "File \u001b[0;32m~/Documents/Banco de dados/Avaliação 1/.venv/lib/python3.11/site-packages/pyspark/sql/readwriter.py:1984\u001b[0m, in \u001b[0;36mDataFrameWriter.jdbc\u001b[0;34m(self, url, table, mode, properties)\u001b[0m\n\u001b[1;32m   1982\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m properties:\n\u001b[1;32m   1983\u001b[0m     jprop\u001b[38;5;241m.\u001b[39msetProperty(k, properties[k])\n\u001b[0;32m-> 1984\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjprop\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Banco de dados/Avaliação 1/.venv/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Documents/Banco de dados/Avaliação 1/.venv/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/Documents/Banco de dados/Avaliação 1/.venv/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o932.jdbc.\n: org.postgresql.util.PSQLException: ERROR: cannot drop table dim_categories because other objects depend on it\n  Detalhe: constraint dim_categories_fato_sales_items_fk on table fato_sales_items depends on table dim_categories\n  Dica: Use DROP ... CASCADE to drop the dependent objects too.\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2713)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2401)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:368)\n\tat org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:498)\n\tat org.postgresql.jdbc.PgStatement.execute(PgStatement.java:415)\n\tat org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:335)\n\tat org.postgresql.jdbc.PgStatement.executeCachedSql(PgStatement.java:321)\n\tat org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:297)\n\tat org.postgresql.jdbc.PgStatement.executeUpdate(PgStatement.java:270)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.executeStatement(JdbcUtils.scala:1094)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.dropTable(JdbcUtils.scala:81)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:63)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:756)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "postgres_url = \"jdbc:postgresql://localhost:5432/fatorvgestao2\"\n",
    "properties = {\n",
    "    \"user\": \"fatorv\",\n",
    "    \"password\": \"123456\",\n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "    \"url\" : postgres_url\n",
    "}\n",
    "\n",
    "df_categories.write.jdbc(postgres_url, \"dim_categories\", \"overwrite\", properties)\n",
    "df_customers.write.jdbc(postgres_url, \"dim_customers\", \"overwrite\", properties)\n",
    "df_sellers.write.jdbc(postgres_url, \"dim_sellers\", \"overwrite\", properties)\n",
    "df_suppliers.write.jdbc(postgres_url, \"dim_suppliers\", \"overwrite\", properties)\n",
    "df_date.write.jdbc(postgres_url, \"dim_date\", \"overwrite\", properties)\n",
    "df_states.write.jdbc(postgres_url, \"dim_states\", \"overwrite\", properties)\n",
    "df_products.write.jdbc(postgres_url, \"dim_products\", \"overwrite\", properties)\n",
    "df_joined_prod_sup_cat_si_sa.write.jdbc(postgres_url, \"fato_sales_items\", \"overwrite\", properties)\n",
    "\n",
    "# Fecha a sessão do Spark\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
